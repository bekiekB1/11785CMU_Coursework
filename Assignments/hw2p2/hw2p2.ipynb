{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "25Gb_ram_cmuhw2p2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GXXnwIqS1_U",
        "outputId": "eafeb3f8-a083-4ba6-c8ed-593d62a39e8f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sUeS2moS5Nv"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "from torchvision import transforms, datasets, models\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.utils import data\r\n",
        "from collections import namedtuple\r\n",
        "from IPython.display import Image\r\n",
        "import time\r\n",
        "%matplotlib inline\r\n",
        "np.random.seed(2020)\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqG5kvyYS_jA"
      },
      "source": [
        "#Hyperparameter for tuining(Only MLP)\r\n",
        "hyper = {\r\n",
        "        \"nEpochs\":5,\r\n",
        "        \"lr\":0.1,\r\n",
        "        \"kcontext\":12,\r\n",
        "        \"lr_decayRate\":0.0,\r\n",
        "        \"randomSeed\":2021,\r\n",
        "        \"batchSize\":256,\r\n",
        "        \"dataPath\":'/content/gdrive/MyDrive/Frame-Level_Classification_of_Speech/',\r\n",
        "        \"weightDirName\": './checkpoint/',\r\n",
        "        \"hiddenDims\": [2048,1024,1024,512],\r\n",
        "        \"checkpointPath\":\"./checkpoint/model_5.txt\"\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJhf_5IxTBL-"
      },
      "source": [
        "train_data = (np.load(hyper['dataPath']+\"/train.npy\", allow_pickle=True), \r\n",
        "        np.load(hyper['dataPath']+\"/train_labels.npy\", allow_pickle=True))\r\n",
        "dev_data = (np.load(hyper['dataPath']+\"/dev.npy\", allow_pickle=True), \r\n",
        "        np.load(hyper['dataPath']+\"/dev_labels.npy\", allow_pickle=True))\r\n",
        "test_data = (np.load(hyper['dataPath']+\"/test.npy\", allow_pickle=True), \r\n",
        "        None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8stRrzPQTCyd"
      },
      "source": [
        "class MyDataset(data.Dataset):\r\n",
        "    def __init__(self, dataset, k):\r\n",
        "        self.k = k\r\n",
        "        self.dataX = dataset[0]\r\n",
        "        self.dataY = dataset[1] if len(dataset) == 2 else None\r\n",
        "        self.idxMap = []\r\n",
        "        for i, utter in enumerate(self.dataX):\r\n",
        "            for j in range(utter.shape[0]):\r\n",
        "                self.idxMap.append((i, j)) # frame index, each frame has dim 40\r\n",
        "        \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        i, j = self.idxMap[idx]\r\n",
        "        withContext = self.dataX[i].take(range(j - self.k, j + self.k + 1), mode='clip', axis=0).flatten()\r\n",
        "        x = torch.Tensor(withContext).float()\r\n",
        "        y = self.dataY[i][j] if self.dataY is not None else -1\r\n",
        "        return x, y\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.idxMap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI52Ypj0TEkG"
      },
      "source": [
        "\r\n",
        "#num_workers = 8 if cuda else 0 \r\n",
        "num_workers = 0\r\n",
        "# Training\r\n",
        "train_dataset = MyDataset(train_data, hyper['kcontext'])\r\n",
        "\r\n",
        "train_loader_args = dict(shuffle=True, batch_size=hyper['batchSize'], num_workers=num_workers, pin_memory=True) if cuda\\\r\n",
        "                    else dict(shuffle=True, batch_size=64)\r\n",
        "train_loader = data.DataLoader(train_dataset, **train_loader_args,drop_last=True)\r\n",
        "\r\n",
        "#Validation\r\n",
        "val_dataset = MyDataset(dev_data, hyper['kcontext'])\r\n",
        "\r\n",
        "val_loader_args = dict(shuffle=True, batch_size=hyper['batchSize'], num_workers=num_workers, pin_memory=True) if cuda\\\r\n",
        "                    else dict(shuffle=True, batch_size=64)\r\n",
        "val_loader = data.DataLoader(val_dataset, **train_loader_args)\r\n",
        "\r\n",
        "# Testing\r\n",
        "test_dataset = MyDataset(test_data, hyper['kcontext'])\r\n",
        "\r\n",
        "test_loader_args = dict(shuffle=False, batch_size=hyper['batchSize'], num_workers=num_workers, pin_memory=True) if cuda\\\r\n",
        "                    else dict(shuffle=False, batch_size=1)\r\n",
        "test_loader = data.DataLoader(test_dataset, **test_loader_args)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1V6XPUgTG1m"
      },
      "source": [
        "class Simple_MLP(nn.Module):\r\n",
        "    def __init__(self, size_list):\r\n",
        "        super(Simple_MLP, self).__init__()\r\n",
        "        layers = []\r\n",
        "        self.size_list = size_list\r\n",
        "        for i in range(len(size_list) - 2):\r\n",
        "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\r\n",
        "            layers.append(nn.BatchNorm1d(size_list[i+1]))\r\n",
        "            layers.append(nn.ReLU())\r\n",
        "            layers.append(nn.Dropout(0.5))\r\n",
        "        layers.append(nn.Linear(size_list[-2], size_list[-1]))\r\n",
        "        self.net = nn.Sequential(*layers)\r\n",
        "        #self.layers = nn.ModuleList(layers)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwAQy12LqLOY"
      },
      "source": [
        "def one_epoch(epoch, net, loader, optimizer):\r\n",
        "    net.train()\r\n",
        "    running_loss = 0.0\r\n",
        "    n = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    start_time = time.time()\r\n",
        "    for i, data in enumerate(loader):\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "        inputs, labels = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        # forward + backward + optimize\r\n",
        "        inupts = inputs.squeeze(0)\r\n",
        "        outputs = net(inputs)\r\n",
        "        loss = F.cross_entropy(outputs, labels)\r\n",
        "        running_loss += loss.item()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "        #print(f\"\\nFinished batch {i+1}  \\t Timestamp: {time.time() - start_time}\")\r\n",
        "        if i % 5000 == 4999:\r\n",
        "            print(f\"\\nFinished batch {i+1}  \\t Timestamp: {time.time() - start_time}\")\r\n",
        "        #TODO: Here\r\n",
        "        #if i % 10 == 9:\r\n",
        "        #    print(f\"\\nFinished batch {i}  \\t Timestamp: {time.time() - start_time}\")\r\n",
        "        #print(f\"\\nFinished batch {i}  \\t Timestamp: {time.time() - start_time}\")\r\n",
        "        # print statistics\r\n",
        "    avg_loss = running_loss / total\r\n",
        "    acc = correct / total *100\r\n",
        "    end_time = time.time()\r\n",
        "    #print(f'\\n\\n++++++++++++++Epoch done++++++++++++\\n')\r\n",
        "    #print('Training Loss: %.8f \\t Time: {%.3f}s\\n\\n'%(avg_loss, end_time - start_time))\r\n",
        "    return avg_loss, acc\r\n",
        "\r\n",
        "def train_step(net, loader, dev_loader, optimizer, scheduler, epochs):\r\n",
        "    net.train()\r\n",
        "    train_losses = []\r\n",
        "    valid_losses = []\r\n",
        "    valid_acc = []\r\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\r\n",
        "        avg_loss_t, acc_t = one_epoch(epoch, net, loader, optimizer)\r\n",
        "        avg_loss_v, acc_v = infer(net, dev_loader)\r\n",
        "        #scheduler.step() # StepLR\r\n",
        "        scheduler.step(avg_loss_v)\r\n",
        "        print(f'\\n\\n*************\\n')\r\n",
        "        print('Epoch [%d], loss: %.8f, acc: %.4f' %\r\n",
        "                (epoch + 1, avg_loss_t, acc_t))\r\n",
        "        print('[valid] loss: %.8f, acc: %.4f\\n\\n' % (avg_loss_v, acc_v))\r\n",
        "        #print(\"lr: {}\".format(optimizer.param_groups[0]['lr']))\r\n",
        "        '''\r\n",
        "        if epoch % 5 == 4:\r\n",
        "            print(f'\\n\\n*************\\n')\r\n",
        "            print('Epoch [%d], loss: %.8f, acc: %.4f' %\r\n",
        "                  (epoch + 1, avg_loss_t, acc_t))\r\n",
        "            print('[valid] loss: %.8f, acc: %.4f\\n\\n' % (avg_loss_v, acc_v))\r\n",
        "            #print(\"lr: {}\".format(optimizer.param_groups[0]['lr']))\r\n",
        "        '''  \r\n",
        "        train_losses.append(avg_loss_t)\r\n",
        "        valid_losses.append(avg_loss_v)\r\n",
        "        valid_acc.append(acc_v)\r\n",
        "        print('\\n','='*20)\r\n",
        "        print(\"*** Saving Checkpoint ***\\n\")\r\n",
        "        #path = \"{}Model_Epoch_{}_v1.txt\".format(hyper['weightDirName'], str(epoch))\r\n",
        "        path = \"gdrive/MyDrive/Bibek/\"+\"Model_\"+str(epoch) #colab\r\n",
        "        torch.save({\r\n",
        "            'epoch': epoch,\r\n",
        "            'model_state_dict': net.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'train_loss': train_losses,\r\n",
        "            'dev_loss':valid_losses,\r\n",
        "            'dev_acc': valid_acc\r\n",
        "        }, path)\r\n",
        "        \r\n",
        "    return train_losses, valid_losses,valid_acc\r\n",
        "\r\n",
        "# Xavier\r\n",
        "def init_xavier(m):\r\n",
        "    if type(m) == nn.Linear:\r\n",
        "        fan_in = m.weight.size()[1]\r\n",
        "        fan_out = m.weight.size()[0]\r\n",
        "        std = np.sqrt(2.0 / (fan_in + fan_out))\r\n",
        "        m.weight.data.normal_(0,std)\r\n",
        "\r\n",
        "def train(net, loader, dev_loader, optimizer, epochs):\r\n",
        "    net.train()\r\n",
        "    train_losses = []\r\n",
        "    valid_losses = []\r\n",
        "    valid_acc = []\r\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\r\n",
        "        avg_loss_t, acc_t = one_epoch(epoch, net, loader, optimizer)\r\n",
        "        avg_loss_v, acc_v = infer(net, dev_loader)\r\n",
        "        #scheduler.step()\r\n",
        "        print(f'\\n\\n*************\\n')\r\n",
        "        print('Epoch [%d], loss: %.8f, acc: %.4f' %\r\n",
        "                (epoch + 1, avg_loss_t, acc_t))\r\n",
        "        print('[valid] loss: %.8f, acc: %.4f\\n\\n' % (avg_loss_v, acc_v))\r\n",
        "        #print(\"lr: {}\".format(optimizer.param_groups[0]['lr']))\r\n",
        "        '''\r\n",
        "        if epoch % 5 == 4:\r\n",
        "            print(f'\\n\\n*************\\n')\r\n",
        "            print('Epoch [%d], loss: %.8f, acc: %.4f' %\r\n",
        "                  (epoch + 1, avg_loss_t, acc_t))\r\n",
        "            print('[valid] loss: %.8f, acc: %.4f\\n\\n' % (avg_loss_v, acc_v))\r\n",
        "            #print(\"lr: {}\".format(optimizer.param_groups[0]['lr']))\r\n",
        "        '''  \r\n",
        "        train_losses.append(avg_loss_t)\r\n",
        "        valid_losses.append(avg_loss_v)\r\n",
        "        valid_acc.append(acc_v)\r\n",
        "        print('\\n','='*20)\r\n",
        "        print(\"*** Saving Checkpoint ***\\n\")\r\n",
        "        #path = \"{}optimContDecreaseCont_Epoch_{}_v1.txt\".format(hyper['weightDirName'], str(epoch))\r\n",
        "        path = \"gdrive/MyDrive/Bibek/\"+\"Model_\"+str(epoch) #colab\r\n",
        "        torch.save({\r\n",
        "            'epoch': epoch,\r\n",
        "            'model_state_dict': net.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'train_loss': train_losses,\r\n",
        "            'dev_loss':valid_losses,\r\n",
        "            'dev_acc': valid_acc\r\n",
        "        }, path)\r\n",
        "        \r\n",
        "    return train_losses, valid_losses, valid_acc\r\n",
        "\r\n",
        "def infer(net, loader):\r\n",
        "    net.eval()\r\n",
        "    running_loss = 0.0\r\n",
        "    n = 0\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    start_time = time.time()\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, data in enumerate(loader):\r\n",
        "            # get the inputs; data is a list of [inputs, labels]\r\n",
        "            inputs, labels = data\r\n",
        "            inputs = inputs.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "            # zero the parameter gradients\r\n",
        "            # forward + backward + optimize\r\n",
        "            outputs = net(inputs)\r\n",
        "            loss = F.cross_entropy(outputs, labels)\r\n",
        "            running_loss += loss.item()\r\n",
        "            _, predicted = torch.max(outputs.data, 1)\r\n",
        "            total += labels.size(0)\r\n",
        "            correct += (predicted == labels).sum().item()\r\n",
        "            #if i % 1000 == 999:\r\n",
        "            #    print(f\"\\n(Eval)Finished batch{i}  \\t Timestamp: {time.time() - start_time}\")\r\n",
        "\r\n",
        "    acc = correct / total * 100\r\n",
        "    avg_loss = running_loss / total\r\n",
        "    end_time = time.time()\r\n",
        "    return avg_loss, acc\r\n",
        "\r\n",
        "def predictLabels(model, test_loader, device):\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    res = np.array([])\r\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\r\n",
        "        data = data.to(device)\r\n",
        "        target = target.to(device)\r\n",
        "        outputs = model(data)\r\n",
        "        _, predicted = torch.max(outputs.data, dim=1)\r\n",
        "        res = np.concatenate((res, predicted.cpu().numpy().reshape(-1)))\r\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMcgVn1NqLKs"
      },
      "source": [
        "def main(hyper):\r\n",
        "    \"\"\"Main function to run everything with\r\n",
        "    Args: \r\n",
        "\r\n",
        "        hyper(dict): Contains hyperparameter to tweak the model\r\n",
        "    \"\"\"\r\n",
        "    input_size = (2 * hyper[\"kcontext\"] + 1) * 13\r\n",
        "    output_size = 346\r\n",
        "    model = Simple_MLP([input_size] + hyper[\"hiddenDims\"] + [output_size])\r\n",
        "    \r\n",
        "    #For loading the model\r\n",
        "    #checkpoint = torch.load(hyper[\"checkpointPath\"])\r\n",
        "    #model.load_state_dict(checkpoint[\"model_state_dict\"])\r\n",
        "    model.to(device)\r\n",
        "    #Initialize all the weights of linear layer\r\n",
        "    model.apply(init_xavier)\r\n",
        "    AdamOptimizer = torch.optim.Adam(model.parameters(),lr=hyper[\"lr\"])\r\n",
        "    #Lr scheduler\r\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamOptimizer,mode='min',patience=3)\r\n",
        "    #scheduler = torch.optim.lr_scheduler.StepLR(AdamOptimizer, step_size=33, gamma=0.5)\r\n",
        "    print(model)\r\n",
        "\r\n",
        "    train_losses, valid_losses, valid_acc = train_step(model, train_loader, val_loader, AdamOptimizer,scheduler,  hyper['nEpochs'])\r\n",
        "    print(f'\\n\\n$$$$$$$$$Finished$$$$$$$$$$$$$\\n\\n')\r\n",
        "    #train_losses, valid_losses = train(model, train_loader, val_loader, AdamOptimizer, hyper['nEpochs'])\r\n",
        "\r\n",
        "\r\n",
        "    # Pridict label for test dataset\r\n",
        "    labels = predictLabels(model, test_loader, device)\r\n",
        "    np.save(hyper[\"testLabelPath\"], labels)\r\n",
        "    labels = list(map(int, labels))\r\n",
        "    idxs = np.array(list(range(len(labels))))\r\n",
        "    labels = np.array(labels)\r\n",
        "    df = pd.DataFrame({\"id\" : idxs, \"label\" : labels})\r\n",
        "    df.to_csv(hyper[\"testLabelCSVfn\"], index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0B_Oq6DqLHO",
        "outputId": "063daa89-2de7-4db1-a8ee-1533d4216efb"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    main(hyper)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple_MLP(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=325, out_features=2048, bias=True)\n",
            "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.5, inplace=False)\n",
            "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.5, inplace=False)\n",
            "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU()\n",
            "    (15): Dropout(p=0.5, inplace=False)\n",
            "    (16): Linear(in_features=512, out_features=346, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Finished batch 5000  \t Timestamp: 59.27410840988159\n",
            "\n",
            "Finished batch 10000  \t Timestamp: 115.45313477516174\n",
            "\n",
            "Finished batch 15000  \t Timestamp: 171.16501474380493\n",
            "\n",
            "Finished batch 20000  \t Timestamp: 227.1580295562744\n",
            "\n",
            "Finished batch 25000  \t Timestamp: 282.89551281929016\n",
            "\n",
            "Finished batch 30000  \t Timestamp: 338.8403813838959\n",
            "\n",
            "Finished batch 35000  \t Timestamp: 394.99682092666626\n",
            "\n",
            "Finished batch 40000  \t Timestamp: 451.21312642097473\n",
            "\n",
            "Finished batch 45000  \t Timestamp: 506.8474521636963\n",
            "\n",
            "Finished batch 50000  \t Timestamp: 563.1204826831818\n",
            "\n",
            "Finished batch 55000  \t Timestamp: 619.362710237503\n",
            "\n",
            "Finished batch 60000  \t Timestamp: 675.0028190612793\n",
            "\n",
            "Finished batch 65000  \t Timestamp: 730.7253737449646\n",
            "\n",
            "Finished batch 70000  \t Timestamp: 786.6294836997986\n",
            "\n",
            "Finished batch 75000  \t Timestamp: 842.4084825515747\n",
            "\n",
            "Finished batch 80000  \t Timestamp: 899.66943526268\n",
            "\n",
            "Finished batch 85000  \t Timestamp: 956.4270257949829\n",
            "\n",
            "Finished batch 90000  \t Timestamp: 1012.117330789566\n",
            "\n",
            "Finished batch 95000  \t Timestamp: 1068.1537363529205\n",
            "\n",
            "Finished batch 100000  \t Timestamp: 1124.1125011444092\n",
            "\n",
            "Finished batch 105000  \t Timestamp: 1179.6884369850159\n",
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [1], loss: 0.01103105, acc: 30.6445\n",
            "[valid] loss: 0.00870075, acc: 41.9814\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "*** Saving Checkpoint ***\n",
            "\n",
            "\n",
            "Finished batch 5000  \t Timestamp: 59.14516353607178\n",
            "\n",
            "Finished batch 10000  \t Timestamp: 115.05348181724548\n",
            "\n",
            "Finished batch 15000  \t Timestamp: 171.01528930664062\n",
            "\n",
            "Finished batch 20000  \t Timestamp: 227.14727067947388\n",
            "\n",
            "Finished batch 25000  \t Timestamp: 283.5230870246887\n",
            "\n",
            "Finished batch 30000  \t Timestamp: 339.7278437614441\n",
            "\n",
            "Finished batch 35000  \t Timestamp: 395.67645359039307\n",
            "\n",
            "Finished batch 40000  \t Timestamp: 451.9005403518677\n",
            "\n",
            "Finished batch 45000  \t Timestamp: 508.4153048992157\n",
            "\n",
            "Finished batch 50000  \t Timestamp: 564.5648670196533\n",
            "\n",
            "Finished batch 55000  \t Timestamp: 621.1242372989655\n",
            "\n",
            "Finished batch 60000  \t Timestamp: 677.0846025943756\n",
            "\n",
            "Finished batch 65000  \t Timestamp: 733.1169617176056\n",
            "\n",
            "Finished batch 70000  \t Timestamp: 788.8837103843689\n",
            "\n",
            "Finished batch 75000  \t Timestamp: 845.2532258033752\n",
            "\n",
            "Finished batch 80000  \t Timestamp: 901.6799099445343\n",
            "\n",
            "Finished batch 85000  \t Timestamp: 957.6670107841492\n",
            "\n",
            "Finished batch 90000  \t Timestamp: 1013.6673653125763\n",
            "\n",
            "Finished batch 95000  \t Timestamp: 1069.775304555893\n",
            "\n",
            "Finished batch 100000  \t Timestamp: 1125.6534423828125\n",
            "\n",
            "Finished batch 105000  \t Timestamp: 1181.2230460643768\n",
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [2], loss: 0.01073383, acc: 32.0649\n",
            "[valid] loss: 0.00873478, acc: 41.8436\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "*** Saving Checkpoint ***\n",
            "\n",
            "\n",
            "Finished batch 5000  \t Timestamp: 58.89672541618347\n",
            "\n",
            "Finished batch 10000  \t Timestamp: 114.55532670021057\n",
            "\n",
            "Finished batch 15000  \t Timestamp: 170.3247103691101\n",
            "\n",
            "Finished batch 20000  \t Timestamp: 226.2992594242096\n",
            "\n",
            "Finished batch 25000  \t Timestamp: 282.60121488571167\n",
            "\n",
            "Finished batch 30000  \t Timestamp: 338.3443214893341\n",
            "\n",
            "Finished batch 35000  \t Timestamp: 393.95574593544006\n",
            "\n",
            "Finished batch 40000  \t Timestamp: 449.69539856910706\n",
            "\n",
            "Finished batch 45000  \t Timestamp: 505.6793403625488\n",
            "\n",
            "Finished batch 50000  \t Timestamp: 561.5574305057526\n",
            "\n",
            "Finished batch 55000  \t Timestamp: 617.6106095314026\n",
            "\n",
            "Finished batch 60000  \t Timestamp: 673.6539664268494\n",
            "\n",
            "Finished batch 65000  \t Timestamp: 729.7312550544739\n",
            "\n",
            "Finished batch 70000  \t Timestamp: 785.9271945953369\n",
            "\n",
            "Finished batch 75000  \t Timestamp: 841.8518376350403\n",
            "\n",
            "Finished batch 80000  \t Timestamp: 897.8684754371643\n",
            "\n",
            "Finished batch 85000  \t Timestamp: 953.8252201080322\n",
            "\n",
            "Finished batch 90000  \t Timestamp: 1009.2717614173889\n",
            "\n",
            "Finished batch 95000  \t Timestamp: 1064.915712594986\n",
            "\n",
            "Finished batch 100000  \t Timestamp: 1120.6943562030792\n",
            "\n",
            "Finished batch 105000  \t Timestamp: 1176.333357334137\n",
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [3], loss: 0.01074683, acc: 32.0748\n",
            "[valid] loss: 0.00877829, acc: 42.2996\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "*** Saving Checkpoint ***\n",
            "\n",
            "\n",
            "Finished batch 5000  \t Timestamp: 58.482982873916626\n",
            "\n",
            "Finished batch 10000  \t Timestamp: 114.35130310058594\n",
            "\n",
            "Finished batch 15000  \t Timestamp: 170.31853580474854\n",
            "\n",
            "Finished batch 20000  \t Timestamp: 226.06315898895264\n",
            "\n",
            "Finished batch 25000  \t Timestamp: 282.43344807624817\n",
            "\n",
            "Finished batch 30000  \t Timestamp: 338.5799169540405\n",
            "\n",
            "Finished batch 35000  \t Timestamp: 394.79464745521545\n",
            "\n",
            "Finished batch 40000  \t Timestamp: 450.59030652046204\n",
            "\n",
            "Finished batch 45000  \t Timestamp: 506.4629445075989\n",
            "\n",
            "Finished batch 50000  \t Timestamp: 562.2930974960327\n",
            "\n",
            "Finished batch 55000  \t Timestamp: 618.0075161457062\n",
            "\n",
            "Finished batch 60000  \t Timestamp: 673.8357846736908\n",
            "\n",
            "Finished batch 65000  \t Timestamp: 729.4894254207611\n",
            "\n",
            "Finished batch 70000  \t Timestamp: 785.1856713294983\n",
            "\n",
            "Finished batch 75000  \t Timestamp: 840.8381357192993\n",
            "\n",
            "Finished batch 80000  \t Timestamp: 896.4581246376038\n",
            "\n",
            "Finished batch 85000  \t Timestamp: 952.164412021637\n",
            "\n",
            "Finished batch 90000  \t Timestamp: 1007.7788105010986\n",
            "\n",
            "Finished batch 95000  \t Timestamp: 1063.5374569892883\n",
            "\n",
            "Finished batch 100000  \t Timestamp: 1119.532663345337\n",
            "\n",
            "Finished batch 105000  \t Timestamp: 1175.2302632331848\n",
            "\n",
            "\n",
            "*************\n",
            "\n",
            "Epoch [4], loss: 0.01068383, acc: 32.4050\n",
            "[valid] loss: 0.00830286, acc: 44.6286\n",
            "\n",
            "\n",
            "\n",
            " ====================\n",
            "*** Saving Checkpoint ***\n",
            "\n",
            "\n",
            "Finished batch 5000  \t Timestamp: 58.80316972732544\n",
            "\n",
            "Finished batch 10000  \t Timestamp: 114.72250366210938\n",
            "\n",
            "Finished batch 15000  \t Timestamp: 170.83058214187622\n",
            "\n",
            "Finished batch 20000  \t Timestamp: 226.8105185031891\n",
            "\n",
            "Finished batch 25000  \t Timestamp: 282.73858737945557\n",
            "\n",
            "Finished batch 30000  \t Timestamp: 338.7065453529358\n",
            "\n",
            "Finished batch 35000  \t Timestamp: 394.6428084373474\n",
            "\n",
            "Finished batch 40000  \t Timestamp: 450.7958664894104\n",
            "\n",
            "Finished batch 45000  \t Timestamp: 507.15398120880127\n",
            "\n",
            "Finished batch 50000  \t Timestamp: 562.9616076946259\n",
            "\n",
            "Finished batch 55000  \t Timestamp: 619.1492540836334\n",
            "\n",
            "Finished batch 60000  \t Timestamp: 675.0317974090576\n",
            "\n",
            "Finished batch 65000  \t Timestamp: 730.6621868610382\n",
            "\n",
            "Finished batch 70000  \t Timestamp: 786.4638185501099\n",
            "\n",
            "Finished batch 75000  \t Timestamp: 842.501957654953\n",
            "\n",
            "Finished batch 80000  \t Timestamp: 899.2221455574036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkIuBrtdqLDC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-0DepaoqK-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}